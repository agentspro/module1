"""
ĞœĞ¾Ğ´ÑƒĞ»ÑŒ 1: AI Research Agent Ğ½Ğ° LangChain
Ğ¡Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ° Ğ²ĞµÑ€ÑÑ–Ñ, ÑĞºĞ° Ğ¿Ñ€Ğ°Ñ†ÑÑ” Ğ· Ğ±ÑƒĞ´ÑŒ-ÑĞºĞ¾Ñ Ğ²ĞµÑ€ÑÑ–Ñ”Ñ LangChain
"""

import os
import sys
from datetime import datetime
import json

# ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° Ğ²ĞµÑ€ÑÑ–Ñ— Ñ‚Ğ° Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚
print("ğŸ” ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ… Ğ¿Ğ°ĞºĞµÑ‚Ñ–Ğ²...")

try:
    import langchain
    print(f"âœ… LangChain Ğ²ĞµÑ€ÑÑ–Ñ: {langchain.__version__}")
except ImportError:
    print("âŒ LangChain Ğ½Ğµ Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾")
    sys.exit(1)

try:
    from langchain_openai import ChatOpenAI
    print("âœ… langchain-openai Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾")
except ImportError:
    try:
        from langchain.chat_models import ChatOpenAI
        print("âœ… Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ langchain.chat_models")
    except ImportError:
        print("âŒ ĞĞµ Ğ¼Ğ¾Ğ¶Ñƒ Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ÑƒĞ²Ğ°Ñ‚Ğ¸ ChatOpenAI")
        ChatOpenAI = None

# Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ .env
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… .env Ñ„Ğ°Ğ¹Ğ» Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾")
except:
    print("âš ï¸ python-dotenv Ğ½Ğµ Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾")

# ===========================
# ĞŸĞ ĞĞ¡Ğ¢Ğ˜Ğ™ ĞĞ“Ğ•ĞĞ¢-Ğ”ĞĞ¡Ğ›Ğ†Ğ”ĞĞ˜Ğš
# ===========================

class SimpleResearchAgent:
    """
    ĞŸÑ€Ğ¾ÑÑ‚Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ - Ğ¿Ñ€Ğ°Ñ†ÑÑ” Ğ· Ğ±ÑƒĞ´ÑŒ-ÑĞºĞ¾Ñ Ğ²ĞµÑ€ÑÑ–Ñ”Ñ LangChain
    """
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.has_api = bool(self.api_key)
        
        if self.has_api and ChatOpenAI:
            try:
                self.llm = ChatOpenAI(
                    model="gpt-4",
                    temperature=0.7,
                    api_key=self.api_key
                )
                print(f"âœ… LLM ÑÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¾ Ğ· API ĞºĞ»ÑÑ‡ĞµĞ¼")
            except Exception as e:
                print(f"âš ï¸ ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑŒ ÑÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ¸ LLM: {e}")
                self.llm = None
                self.has_api = False
        else:
            self.llm = None
            print("âš ï¸ ĞŸÑ€Ğ°Ñ†ÑÑ Ğ² Ğ´ĞµĞ¼Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ– (Ğ±ĞµĞ· API)")
    
    def search_web(self, query: str) -> str:
        """ĞŸĞ¾ÑˆÑƒĞº Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ— (Ğ´ĞµĞ¼Ğ¾)"""
        return f"""
ğŸ” Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ Ğ¿Ğ¾ÑˆÑƒĞºÑƒ Ğ´Ğ»Ñ '{query}':

1. **AI Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ÑƒÑ” Ğ¾ÑĞ²Ñ–Ñ‚Ñƒ** - Ğ¨Ñ‚ÑƒÑ‡Ğ½Ğ¸Ğ¹ Ñ–Ğ½Ñ‚ĞµĞ»ĞµĞºÑ‚ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ñ–Ğ¾Ğ½Ñ–Ğ·ÑƒÑ” Ğ¾ÑĞ²Ñ–Ñ‚Ğ½Ñ– Ğ¿Ñ€Ğ¾Ñ†ĞµÑĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ.

2. **Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° 2025** - 85% Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ–Ğ² Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑÑ‚ÑŒ AI-Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¸.

3. **ĞÑĞ½Ğ¾Ğ²Ğ½Ñ– Ñ‚Ñ€ĞµĞ½Ğ´Ğ¸**:
   - ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğµ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ğ¿Ñ–Ğ´Ğ»Ğ°ÑˆÑ‚Ğ¾Ğ²ÑƒÑ”Ñ‚ÑŒÑÑ Ğ¿Ñ–Ğ´ ĞºĞ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°
   - AI-Ñ‚ÑŒÑÑ‚Ğ¾Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ– 24/7
   - ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° Ğ·Ğ°Ğ²Ğ´Ğ°Ğ½ÑŒ ĞµĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ÑŒ Ñ‡Ğ°Ñ Ğ²Ğ¸ĞºĞ»Ğ°Ğ´Ğ°Ñ‡Ñ–Ğ²
   
4. **Ğ’Ğ¸ĞºĞ»Ğ¸ĞºĞ¸**: ĞĞµĞ¾Ğ±Ñ…Ñ–Ğ´Ğ½Ñ–ÑÑ‚ÑŒ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ğ²Ğ¸ĞºĞ»Ğ°Ğ´Ğ°Ñ‡Ñ–Ğ² Ğ½Ğ¾Ğ²Ğ¸Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ñ–ÑĞ¼.

5. **ĞœĞ°Ğ¹Ğ±ÑƒÑ‚Ğ½Ñ”**: ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·ÑƒÑ”Ñ‚ÑŒÑÑ Ğ·Ñ€Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ Ñ€Ğ¸Ğ½ĞºÑƒ EdTech Ğ½Ğ° 45% Ğ´Ğ¾ 2026 Ñ€Ğ¾ĞºÑƒ.
"""
    
    def analyze_sentiment(self, text: str) -> str:
        """ĞĞ½Ğ°Ğ»Ñ–Ğ· Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ñ–"""
        positive_words = ["Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ñ–Ğ¾Ğ½Ñ–Ğ·ÑƒÑ”", "Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ñ", "Ğ·Ñ€Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ", "Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ–", "ĞµĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ÑŒ"]
        negative_words = ["Ğ²Ğ¸ĞºĞ»Ğ¸ĞºĞ¸", "Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¸", "ÑĞºĞ»Ğ°Ğ´Ğ½Ñ–ÑÑ‚ÑŒ"]
        
        text_lower = text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count:
            return "ğŸ“Š Ğ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ: ĞŸĞ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ° (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ñ–ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ³Ğ»ÑĞ´ Ğ½Ğ° AI Ğ² Ğ¾ÑĞ²Ñ–Ñ‚Ñ–)"
        elif neg_count > pos_count:
            return "ğŸ“Š Ğ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ: ĞĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ° (Ğ¿ĞµÑĞ¸Ğ¼Ñ–ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ³Ğ»ÑĞ´)"
        else:
            return "ğŸ“Š Ğ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ: ĞĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ° (Ğ·Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ³Ğ»ÑĞ´)"
    
    def create_report(self, topic: str, search_results: str, sentiment: str) -> str:
        """Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ğ·Ğ²Ñ–Ñ‚Ñƒ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              Ğ—Ğ’Ğ†Ğ¢ Ğ”ĞĞ¡Ğ›Ğ†Ğ”Ğ–Ğ•ĞĞĞ¯ AI-ĞĞ“Ğ•ĞĞ¢Ğ                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“… **Ğ”Ğ°Ñ‚Ğ°**: {timestamp}
ğŸ¯ **Ğ¢ĞµĞ¼Ğ°**: {topic}
ğŸ¤– **ĞĞ³ĞµĞ½Ñ‚**: LangChain Research Agent
ğŸ“Š **Ğ ĞµĞ¶Ğ¸Ğ¼**: {"API" if self.has_api else "Ğ”ĞµĞ¼Ğ¾"}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Œ **Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ˜ ĞŸĞĞ¨Ğ£ĞšĞ£**
{search_results}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ **ĞĞĞĞ›Ğ†Ğ—**
{sentiment}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¡ **Ğ’Ğ˜Ğ¡ĞĞĞ’ĞšĞ˜ Ğ¢Ğ Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ†Ğ‡**

ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ñ– Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ° Ğ·Ñ€Ğ¾Ğ±Ğ¸Ñ‚Ğ¸ Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ñ– Ğ²Ğ¸ÑĞ½Ğ¾Ğ²ĞºĞ¸:

1. **ĞŸĞ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ– Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¸**:
   - AI Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ÑƒÑ” Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ
   - ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ Ñ€ÑƒÑ‚Ğ¸Ğ½Ğ½Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ²Ñ–Ğ»ÑŒĞ½ÑÑ” Ñ‡Ğ°Ñ Ğ´Ğ»Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡Ğ¾ÑÑ‚Ñ–
   - Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ–ÑÑ‚ÑŒ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ğ·Ñ€Ğ¾ÑÑ‚Ğ°Ñ” Ğ·Ğ°Ğ²Ğ´ÑĞºĞ¸ AI-Ğ°ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ğ¼

2. **Ğ’Ğ¸ĞºĞ»Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ–ÑˆĞµĞ½Ğ½Ñ**:
   - ĞĞµĞ¾Ğ±Ñ…Ñ–Ğ´Ğ½Ñ–ÑÑ‚ÑŒ Ğ¿Ñ–Ğ´Ğ²Ğ¸Ñ‰ĞµĞ½Ğ½Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ñ— Ğ³Ñ€Ğ°Ğ¼Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ñ– Ğ²Ğ¸ĞºĞ»Ğ°Ğ´Ğ°Ñ‡Ñ–Ğ²
   - Ğ—Ğ°Ğ±ĞµĞ·Ğ¿ĞµÑ‡ĞµĞ½Ğ½Ñ ĞµÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ğ½Ñ AI
   - Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ğ»ÑĞ´ÑÑŒĞºĞ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñƒ Ğ² Ğ¾ÑĞ²Ñ–Ñ‚Ñ–

3. **Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ñ–Ñ—**:
   - ĞŸĞ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ²Ğµ Ğ²Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ´Ğ¶ĞµĞ½Ğ½Ñ AI-Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ²
   - Ğ†Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ñ–Ñ— Ğ² Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ñƒ
   - Ğ Ğ¾Ğ·Ñ€Ğ¾Ğ±ĞºĞ° Ñ‡Ñ–Ñ‚ĞºĞ¸Ñ… ĞµÑ‚Ğ¸Ñ‡Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ–Ğ²

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… **Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡**: Ğ”Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾
"""
        return report
    
    def research(self, topic: str) -> dict:
        """Ğ’Ğ¸ĞºĞ¾Ğ½Ğ°Ñ‚Ğ¸ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ"""
        print(f"\nğŸš€ ĞŸĞ¾Ñ‡Ğ¸Ğ½Ğ°Ñ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ: {topic}")
        print("=" * 60)
        
        # ĞšÑ€Ğ¾Ğº 1: ĞŸĞ¾ÑˆÑƒĞº
        print("ğŸ“ ĞšÑ€Ğ¾Ğº 1/3: ĞŸĞ¾ÑˆÑƒĞº Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ—...")
        search_results = self.search_web(topic)
        print("   âœ“ Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾")
        
        # ĞšÑ€Ğ¾Ğº 2: ĞĞ½Ğ°Ğ»Ñ–Ğ·
        print("ğŸ“ ĞšÑ€Ğ¾Ğº 2/3: ĞĞ½Ğ°Ğ»Ñ–Ğ· Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ñ–...")
        sentiment = self.analyze_sentiment(search_results)
        print("   âœ“ Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾")
        
        # ĞšÑ€Ğ¾Ğº 3: Ğ—Ğ²Ñ–Ñ‚
        print("ğŸ“ ĞšÑ€Ğ¾Ğº 3/3: Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ·Ğ²Ñ–Ñ‚Ñƒ...")
        report = self.create_report(topic, search_results, sentiment)
        print("   âœ“ Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾")
        
        # Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ
        result = {
            "topic": topic,
            "timestamp": datetime.now().isoformat(),
            "report": report,
            "mode": "api" if self.has_api else "demo"
        }
        
        # Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ Ğ² Ñ„Ğ°Ğ¹Ğ»
        with open("langchain_research_result.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print("\nğŸ’¾ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾: langchain_research_result.json")
        
        return result

# ===========================
# Ğ“ĞĞ›ĞĞ’ĞĞ Ğ¤Ğ£ĞĞšĞ¦Ğ†Ğ¯
# ===========================

def main():
    """Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºÑƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            LANGCHAIN RESEARCH AGENT v2.0                  â•‘
â•‘                  Ğ¡Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ° Ğ²ĞµÑ€ÑÑ–Ñ                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° API ĞºĞ»ÑÑ‡Ğ°
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        print(f"âœ… API ĞºĞ»ÑÑ‡ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {api_key[:10]}...")
    else:
        print("âš ï¸ API ĞºĞ»ÑÑ‡ Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ - Ğ¿Ñ€Ğ°Ñ†ÑÑ Ğ² Ğ´ĞµĞ¼Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ–")
        print("ğŸ’¡ ĞŸÑ–Ğ´ĞºĞ°Ğ·ĞºĞ°: ÑÑ‚Ğ²Ğ¾Ñ€Ñ–Ñ‚ÑŒ .env Ñ„Ğ°Ğ¹Ğ» Ğ· OPENAI_API_KEY=...")
    
    # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
    print("\nğŸ¤– Ğ†Ğ½Ñ–Ñ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°...")
    agent = SimpleResearchAgent(api_key)
    
    # Ğ¢ĞµĞ¼Ğ° Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ
    topic = "Ğ¨Ñ‚ÑƒÑ‡Ğ½Ğ¸Ğ¹ Ñ–Ğ½Ñ‚ĞµĞ»ĞµĞºÑ‚ Ğ² Ğ¾ÑĞ²Ñ–Ñ‚Ñ– 2025: Ñ‚Ñ€ĞµĞ½Ğ´Ğ¸ Ñ‚Ğ° Ğ²Ğ¸ĞºĞ»Ğ¸ĞºĞ¸"
    
    # Ğ’Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ½Ñ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ
    print("\n" + "=" * 60)
    result = agent.research(topic)
    
    # Ğ’Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ
    print("\n" + "=" * 60)
    print(result["report"])
    
    print("\nâœ… ĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾!")
    print("ğŸ“‚ ĞŸĞµÑ€ĞµĞ³Ğ»ÑĞ½ÑŒÑ‚Ğµ Ñ„Ğ°Ğ¹Ğ»: langchain_research_result.json")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡ĞµĞ¼")
    except Exception as e:
        print(f"\nâŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ°: {e}")
        import traceback
        traceback.print_exc()
