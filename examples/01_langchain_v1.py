"""
ĞœĞ¾Ğ´ÑƒĞ»ÑŒ 1: AI Research Agent Ğ½Ğ° LangChain 1.0
Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ” Ğ½Ğ°Ğ¹Ğ½Ğ¾Ğ²Ñ–ÑˆÑ– Ğ²ĞµÑ€ÑÑ–Ñ— Ğ²ÑÑ–Ñ… Ğ¿Ğ°ĞºĞµÑ‚Ñ–Ğ²
"""

import os
import sys
from datetime import datetime
import json
from typing import Dict, List, Any

# Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ .env
try:
    from dotenv import load_dotenv
    load_dotenv()
    print(".env Ñ„Ğ°Ğ¹Ğ» Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾")
except:
    print("python-dotenv Ğ½Ğµ Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾")

# Ğ†Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸ Ğ´Ğ»Ñ LangChain 1.0
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    print("LangChain 1.0 ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¸ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾")
except ImportError as e:
    print(f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñƒ LangChain: {e}")
    print("Ğ’ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ñ–Ñ‚ÑŒ: pip install langchain-openai langchain-core")
    sys.exit(1)

# ===========================
# LANGCHAIN 1.0 AGENT
# ===========================

class LangChain1Agent:
    """
    ĞĞ³ĞµĞ½Ñ‚-Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ½Ğ¸Ğº Ğ½Ğ° LangChain 1.0
    Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ” Ğ½Ğ¾Ğ²Ñƒ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ LCEL (LangChain Expression Language)
    """
    
    def __init__(self, api_key: str = None):
        """Ğ†Ğ½Ñ–Ñ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        
        if not self.api_key:
            print("OPENAI_API_KEY Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾!")
            self.llm = None
        else:
            try:
                # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ LLM Ğ´Ğ»Ñ LangChain 1.0
                self.llm = ChatOpenAI(
                    model="gpt-4",
                    temperature=0.7,
                    api_key=self.api_key
                )
                print(f" ChatOpenAI LLM ÑÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¾")
            except Exception as e:
                print(f" ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° ÑÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ LLM: {e}")
                self.llm = None
        
        # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ²
        self.tools = self._create_tools()
        
        # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ğ»Ğ°Ğ½Ñ†ÑĞ³Ñ–Ğ² (chains) - Ğ½Ğ¾Ğ²Ğ° Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LangChain 1.0
        self.chains = self._create_chains()
    
    def _create_tools(self) -> Dict:
        """Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ"""
        
        def search_web(query: str) -> str:
            """ĞŸĞ¾ÑˆÑƒĞº Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ— Ğ² Ñ–Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ñ–"""
            try:
                from ddgs import DDGS
                with DDGS() as ddgs:
                    results = list(ddgs.text(query, max_results=3))
                    if results:
                        output = "ğŸ” Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ Ğ¿Ğ¾ÑˆÑƒĞºÑƒ:\n\n"
                        for i, r in enumerate(results, 1):
                            output += f"{i}. {r.get('title', '')}\n"
                            output += f"   {r.get('body', '')[:200]}...\n"
                            output += f"   {r.get('href', '')}\n\n"
                        return output
            except Exception as e:
                print(f" ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ğ¾ÑˆÑƒĞºÑƒ: {e}")
            
            # Ğ”ĞµĞ¼Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
            return f"""
Ğ”ĞµĞ¼Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ Ğ´Ğ»Ñ '{query}':

1. AI Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ÑƒÑ” Ğ¾ÑĞ²Ñ–Ñ‚Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ
   Ğ¨Ñ‚ÑƒÑ‡Ğ½Ğ¸Ğ¹ Ñ–Ğ½Ñ‚ĞµĞ»ĞµĞºÑ‚ Ğ´Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ” ÑÑ‚Ğ²Ğ¾Ñ€ÑĞ²Ğ°Ñ‚Ğ¸ Ñ–Ğ½Ğ´Ğ¸Ğ²Ñ–Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ– Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ»ÑŒĞ½Ñ– Ñ‚Ñ€Ğ°Ñ”ĞºÑ‚Ğ¾Ñ€Ñ–Ñ—...

2. Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° 2025: 85% Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ–Ğ² Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑÑ‚ÑŒ AI
   Ğ—Ğ° Ğ´Ğ°Ğ½Ğ¸Ğ¼Ğ¸ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ, Ğ±Ñ–Ğ»ÑŒÑˆÑ–ÑÑ‚ÑŒ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ–Ğ² Ğ²Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ´Ğ¸Ğ»Ğ¸ AI-Ñ€Ñ–ÑˆĞµĞ½Ğ½Ñ...

3. Ğ’Ğ¸ĞºĞ»Ğ¸ĞºĞ¸ Ğ²Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ´Ğ¶ĞµĞ½Ğ½Ñ AI Ğ² Ğ¾ÑĞ²Ñ–Ñ‚Ñ–
   ĞÑĞ½Ğ¾Ğ²Ğ½Ñ– Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¸: Ğ¿Ñ–Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ĞºĞ°Ğ´Ñ€Ñ–Ğ², ĞµÑ‚Ğ¸Ñ‡Ğ½Ñ– Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ–ÑÑ‚ÑŒ...
"""
        
        def analyze_data(text: str) -> str:
            """ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ´Ğ°Ğ½Ğ¸Ñ…"""
            word_count = len(text.split())
            sentences = text.count('.') + text.count('!') + text.count('?')
            
            # ĞŸÑ€Ğ¾ÑÑ‚Ğ¸Ğ¹ sentiment Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·
            positive = ["ÑƒÑĞ¿Ñ–Ñ…", "Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ñ", "Ñ–Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ñ–Ñ", "Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑ", "ĞµÑ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ–ÑÑ‚ÑŒ"]
            negative = ["Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°", "Ğ²Ğ¸ĞºĞ»Ğ¸Ğº", "Ñ€Ğ¸Ğ·Ğ¸Ğº", "Ğ·Ğ°Ğ³Ñ€Ğ¾Ğ·Ğ°", "ÑĞºĞ»Ğ°Ğ´Ğ½Ñ–ÑÑ‚ÑŒ"]
            
            pos_count = sum(1 for word in positive if word in text.lower())
            neg_count = sum(1 for word in negative if word in text.lower())
            
            sentiment = "Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹" if pos_count > neg_count else "Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹" if neg_count > pos_count else "Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹"
            
            return f"""
ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ´Ğ°Ğ½Ğ¸Ñ…:
- Ğ¡Ğ»Ñ–Ğ²: {word_count}
- Ğ ĞµÑ‡ĞµĞ½ÑŒ: {sentences}
- Ğ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ: {sentiment}
- ĞŸĞ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¸Ñ… Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ–Ğ²: {pos_count}
- ĞĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¸Ñ… Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ–Ğ²: {neg_count}
"""
        
        def save_to_memory(data: dict) -> str:
            """Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ğ² Ğ¿Ğ°Ğ¼'ÑÑ‚ÑŒ"""
            filename = "langchain1_memory.json"
            try:
                with open(filename, 'r') as f:
                    memory = json.load(f)
            except:
                memory = {"sessions": []}
            
            memory["sessions"].append(data)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(memory, f, ensure_ascii=False, indent=2)
            
            return f"Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾ Ğ² {filename}"
        
        return {
            "search_web": search_web,
            "analyze_data": analyze_data,
            "save_to_memory": save_to_memory
        }
    
    def _create_chains(self):
        """Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ğ»Ğ°Ğ½Ñ†ÑĞ³Ñ–Ğ² Ğ´Ğ»Ñ LangChain 1.0 LCEL"""
        chains = {}
        
        if self.llm:
            # ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ
            research_prompt = ChatPromptTemplate.from_messages([
                ("system", "Ğ’Ğ¸ - Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑ–Ğ¹Ğ½Ğ¸Ğ¹ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ½Ğ¸Ğº AI Ğ² Ğ¾ÑĞ²Ñ–Ñ‚Ñ–. ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ¹Ñ‚Ğµ Ğ½Ğ°Ğ´Ğ°Ğ½Ñƒ Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ Ñ‚Ğ° ÑÑ‚Ğ²Ğ¾Ñ€Ñ–Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ·Ğ²Ñ–Ñ‚."),
                ("human", "Ğ¢ĞµĞ¼Ğ°: {topic}\n\nĞ”Ğ°Ğ½Ñ–:\n{data}\n\nĞ¡Ñ‚Ğ²Ğ¾Ñ€Ñ–Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·.")
            ])
            
            # Ğ›Ğ°Ğ½Ñ†ÑĞ³ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ (LCEL ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ)
            chains["research"] = research_prompt | self.llm | StrOutputParser()
            
            # Ğ›Ğ°Ğ½Ñ†ÑĞ³ Ğ´Ğ»Ñ Ğ²Ğ¸ÑĞ½Ğ¾Ğ²ĞºÑ–Ğ²
            conclusion_prompt = ChatPromptTemplate.from_messages([
                ("system", "Ğ’Ğ¸ ĞµĞºÑĞ¿ĞµÑ€Ñ‚ Ğ· Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ²Ğ¸ÑĞ½Ğ¾Ğ²ĞºÑ–Ğ². Ğ£Ğ·Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ñ–Ñ‚ÑŒ Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ."),
                ("human", "{analysis}")
            ])
            
            chains["conclusion"] = conclusion_prompt | self.llm | StrOutputParser()
        
        return chains
    
    async def research_async(self, topic: str) -> dict:
        """ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ (Ğ½Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ LangChain 1.0)"""
        # LangChain 1.0 Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ” async Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ñ–Ñ—
        pass
    
    def research(self, topic: str) -> dict:
        """Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ´Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ"""
        print(f"\nLangChain 1.0 Agent: Ğ”Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ '{topic}'")
        print("=" * 60)
        
        results = {"topic": topic, "timestamp": datetime.now().isoformat()}
        
        # ĞšÑ€Ğ¾Ğº 1: ĞŸĞ¾ÑˆÑƒĞº
        print("ĞšÑ€Ğ¾Ğº 1: ĞŸĞ¾ÑˆÑƒĞº Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ—...")
        search_results = self.tools["search_web"](topic)
        results["search"] = search_results
        print("   Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾")
        
        # ĞšÑ€Ğ¾Ğº 2: ĞĞ½Ğ°Ğ»Ñ–Ğ·
        print("ĞšÑ€Ğ¾Ğº 2: ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ´Ğ°Ğ½Ğ¸Ñ…...")
        analysis = self.tools["analyze_data"](search_results)
        results["analysis"] = analysis
        print("   Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾")
        
        # ĞšÑ€Ğ¾Ğº 3: AI Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ° (ÑĞºÑ‰Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°)
        if self.llm and "research" in self.chains:
            print("ĞšÑ€Ğ¾Ğº 3: AI Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·...")
            try:
                ai_analysis = self.chains["research"].invoke({
                    "topic": topic,
                    "data": search_results
                })
                results["ai_analysis"] = ai_analysis
                print("   Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾")
            except Exception as e:
                print(f"   ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° AI: {e}")
                results["ai_analysis"] = "AI Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¸Ğ¹"
        else:
            print(" ĞšÑ€Ğ¾Ğº 3: Ğ”ĞµĞ¼Ğ¾ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·...")
            results["ai_analysis"] = self._demo_analysis(topic)
            print("   Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾")
        
        # ĞšÑ€Ğ¾Ğº 4: Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ
        print(" ĞšÑ€Ğ¾Ğº 4: Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ²...")
        save_result = self.tools["save_to_memory"](results)
        print(f"   {save_result}")
        
        # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ğ·Ğ²Ñ–Ñ‚Ñƒ
        report = self._create_report(results)
        results["report"] = report
        
        # Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ñ„Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ²Ñ–Ñ‚Ñƒ
        with open("langchain1_report.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print("\nĞŸĞ¾Ğ²Ğ½Ğ¸Ğ¹ Ğ·Ğ²Ñ–Ñ‚: langchain1_report.json")
        
        return results
    
    def _demo_analysis(self, topic: str) -> str:
        """Ğ”ĞµĞ¼Ğ¾ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ğ´Ğ»Ñ Ğ²Ğ¸Ğ¿Ğ°Ğ´ĞºÑ–Ğ² Ğ±ĞµĞ· API"""
        return f"""
ĞĞ½Ğ°Ğ»Ñ–Ğ· Ñ‚ĞµĞ¼Ğ¸ '{topic}':

**ĞÑĞ½Ğ¾Ğ²Ğ½Ñ– Ñ‚Ñ€ĞµĞ½Ğ´Ğ¸:**
â€¢ ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ñ‡ĞµÑ€ĞµĞ· AI
â€¢ ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ Ñ€ÑƒÑ‚Ğ¸Ğ½Ğ½Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡
â€¢ ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ– Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ»ÑŒĞ½Ñ– ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸

**ĞŸĞµÑ€ĞµĞ²Ğ°Ğ³Ğ¸:**
â€¢ ĞŸÑ–Ğ´Ğ²Ğ¸Ñ‰ĞµĞ½Ğ½Ñ ĞµÑ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ñ– Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ğ½Ğ° 30%
â€¢ Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ–ÑÑ‚ÑŒ 24/7
â€¢ Ğ†Ğ½Ğ´Ğ¸Ğ²Ñ–Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ğ¿Ñ–Ğ´Ñ…Ñ–Ğ´

**Ğ’Ğ¸ĞºĞ»Ğ¸ĞºĞ¸:**
â€¢ ĞĞµĞ¾Ğ±Ñ…Ñ–Ğ´Ğ½Ñ–ÑÑ‚ÑŒ Ğ¿Ñ–Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ¸ĞºĞ»Ğ°Ğ´Ğ°Ñ‡Ñ–Ğ²
â€¢ ĞŸĞ¸Ñ‚Ğ°Ğ½Ğ½Ñ ĞµÑ‚Ğ¸ĞºĞ¸ Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ñ–
â€¢ Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ° Ğ½ĞµÑ€Ñ–Ğ²Ğ½Ñ–ÑÑ‚ÑŒ

**ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·:**
ĞÑ‡Ñ–ĞºÑƒÑ”Ñ‚ÑŒÑÑ Ğ·Ñ€Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ Ñ€Ğ¸Ğ½ĞºÑƒ EdTech Ğ½Ğ° 45% Ğ´Ğ¾ 2026 Ñ€Ğ¾ĞºÑƒ.
"""
    
    def _create_report(self, results: dict) -> str:
        """Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ñ„Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ²Ñ–Ñ‚Ñƒ"""
        return f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              LANGCHAIN 1.0 RESEARCH REPORT                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ğ”Ğ°Ñ‚Ğ°: {results['timestamp']}
Ğ¢ĞµĞ¼Ğ°: {results['topic']}
ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°: LangChain 1.0 + OpenAI GPT-4

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ˜ ĞŸĞĞ¨Ğ£ĞšĞ£:
{results.get('search', 'ĞĞµĞ¼Ğ°Ñ” Ğ´Ğ°Ğ½Ğ¸Ñ…')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜Ğ§ĞĞ˜Ğ™ ĞĞĞĞ›Ğ†Ğ—:
{results.get('analysis', 'ĞĞµĞ¼Ğ°Ñ” Ğ´Ğ°Ğ½Ğ¸Ñ…')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AI ĞĞĞĞ›Ğ†Ğ¢Ğ˜ĞšĞ:
{results.get('ai_analysis', 'ĞĞµĞ¼Ğ°Ñ” Ğ´Ğ°Ğ½Ğ¸Ñ…')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ğ”Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾ ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾
"""

# ===========================
# Ğ“ĞĞ›ĞĞ’ĞĞ Ğ¤Ğ£ĞĞšĞ¦Ğ†Ğ¯
# ===========================

def main():
    """Ğ—Ğ°Ğ¿ÑƒÑĞº LangChain 1.0 Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            LANGCHAIN 1.0 RESEARCH AGENT                      â•‘
â•‘                ĞĞ°Ğ¹Ğ½Ğ¾Ğ²Ñ–ÑˆÑ– Ğ²ĞµÑ€ÑÑ–Ñ— Ğ¿Ğ°ĞºĞµÑ‚Ñ–                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ° Ğ²ĞµÑ€ÑÑ–Ğ¹
    print("Ğ’ĞµÑ€ÑÑ–Ñ— Ğ¿Ğ°ĞºĞµÑ‚Ñ–Ğ²:")
    try:
        import langchain
        print(f"   LangChain: {langchain.__version__}")
    except:
        print(f"   LangChain: Ğ½Ğµ Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾")
    
    try:
        import langchain_openai
        print(f"   LangChain-OpenAI: Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾")
    except:
        print(f"   LangChain-OpenAI: Ğ½Ğµ Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾")
    
    try:
        import openai
        print(f"   OpenAI: {openai.__version__}")
    except:
        print(f"   OpenAI: Ğ½Ğµ Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾")
    
    # API ĞºĞ»ÑÑ‡
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        print(f"   API ĞºĞ»ÑÑ‡: {api_key[:10]}...{api_key[-4:]}")
    else:
        print(f"   API ĞºĞ»ÑÑ‡: Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ (Ğ´ĞµĞ¼Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼)")
    
    print("\n" + "=" * 60)
    
    # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
    agent = LangChain1Agent(api_key)
    
    # Ğ”Ğ¾ÑĞ»Ñ–Ğ´Ğ¶ĞµĞ½Ğ½Ñ
    topic = "Ğ¨Ñ‚ÑƒÑ‡Ğ½Ğ¸Ğ¹ Ñ–Ğ½Ñ‚ĞµĞ»ĞµĞºÑ‚ Ğ² Ğ¾ÑĞ²Ñ–Ñ‚Ñ– 2025: Ğ½Ğ°Ğ¹Ğ½Ğ¾Ğ²Ñ–ÑˆÑ– Ñ‚Ñ€ĞµĞ½Ğ´Ğ¸"
    result = agent.research(topic)
    
    # Ğ’Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ Ğ·Ğ²Ñ–Ñ‚Ñƒ
    print("\n" + "=" * 60)
    print(result["report"])
    
    print("\nĞ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! ĞŸĞµÑ€ĞµĞ³Ğ»ÑĞ½ÑŒÑ‚Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¸:")
    print("   â€¢ langchain1_report.json - Ğ¿Ğ¾Ğ²Ğ½Ñ– Ğ´Ğ°Ğ½Ñ–")
    print("   â€¢ langchain1_memory.json - Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ° Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾")
    except Exception as e:
        print(f"\nĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°: {e}")
        import traceback
        traceback.print_exc()
